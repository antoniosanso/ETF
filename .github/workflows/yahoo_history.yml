name: ETF Historical Prices (from 2010-01-01)

on:
  workflow_dispatch:
  schedule:
    - cron: "17 6 * * 1-5"

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  fetch-history:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Fetch history from Yahoo Finance
        env:
          ETF_CSV: ETF.csv
          MAP_CSV: yahoo_map.csv
          OUT_CSV: datasets/investing_history.csv
          YAHOO_SUFFIX_ORDER: ".MI,.AS,.PA,.DE,.IR,"   # .MI PRIMA DI TUTTO
        run: python scripts/fetch_yahoo_v4.py

      - name: Commit dataset
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add datasets/investing_history.csv
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Update investing_history.csv (Yahoo, auto)"
            git push
          fi

      - name: Upload artifact (history CSV)
        uses: actions/upload-artifact@v4
        with:
          name: investing_history_csv
          path: datasets/investing_history.csv

      # ---------- GitHub Pages publication ----------
      - name: Configure GitHub Pages
        uses: actions/configure-pages@v5

      - name: Prepare GitHub Pages artifact
        run: |
          mkdir -p public/datasets
          # Copia il CSV "unico"
          cp datasets/investing_history.csv public/datasets/

          # Genera HTML globale + un HTML per ogni ETF (ticker)
          python - << 'PY'
          import os
          import pandas as pd

          csv_path = "datasets/investing_history.csv"
          out_dir = "public/datasets"
          os.makedirs(out_dir, exist_ok=True)
          
          df = pd.read_csv(csv_path)
          
          # Normalizza nomi colonne
          df.columns = [str(c).lower() for c in df.columns]
          if "symbol" in df.columns and "ticker" not in df.columns:
              df = df.rename(columns={"symbol": "ticker"})
          
          # HTML complessivo (opzionale, lo manteniamo)
          full_html_path = os.path.join(out_dir, "investing_history.html")
          df.to_html(full_html_path, index=False)
          
          # HTML per singolo ETF (ticker)
          if "ticker" not in df.columns:
              raise ValueError(f"Nessuna colonna 'ticker' trovata. Colonne: {list(df.columns)}")
          
          # Se c'Ã¨ una colonna 'date' la usiamo per ordinare
          date_col = "date" if "date" in df.columns else None
          
          tickers = sorted(t for t in df["ticker"].dropna().unique())
          for t in tickers:
              sdf = df[df["ticker"] == t].copy()
              if date_col is not None and date_col in sdf.columns:
                  sdf[date_col] = pd.to_datetime(sdf[date_col], errors="coerce")
                  sdf = sdf.sort_values(date_col)
              out_path = os.path.join(out_dir, f"{t}.html")
              sdf.to_html(out_path, index=False)
          PY

          # Disabilita cache lato CDN su Pages per la cartella datasets
          echo "/datasets/*" > public/_headers
          echo "  Cache-Control: no-cache, no-store, must-revalidate" >> public/_headers

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
